{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='index'></a>\n",
    "\n",
    "# Index\n",
    "\n",
    "## Cofiguration\n",
    "\n",
    "[1. AWS Credentials](#aws_credentials)\n",
    "\n",
    "[2. Clustering Functions](#clustering-funcs)\n",
    "\n",
    "\n",
    "\n",
    "## Cluster models\n",
    "\n",
    "[1. Behavioural](#behave)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%local\n",
    "!pip install ipython-autotime\n",
    "!pip install pyathena\n",
    "%load_ext autotime\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "print( \"Switched to:\",matplotlib.get_backend())\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import lit\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input Ophan AWS Credentials below\n",
    "\n",
    "<a id='aws_credentials'></a>\n",
    "\n",
    "[Back to index](#index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get your ophan credentials and paste in below\n",
    "\n",
    "access_key = ''\n",
    "secret_key = ''\n",
    "token = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "!aws s3 ls s3://ophan-temp-schema/data/dusvyat/project_heart/ --profile ophan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark ML K Means clustering\n",
    "\n",
    "<a id='clustering-funcs'></a>\n",
    "\n",
    "[Back to index](#index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions for clustering\n",
    "[Back to index](#index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_df(access_key,secret_key,token,fname):\n",
    "    \n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key) \n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\", token)\n",
    "\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "\n",
    "    spark._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    \n",
    "    df=sqlContext.read.option(\"delimiter\", \",\") \\\n",
    ".orc(\"s3a://ophan-temp-schema/data/dusvyat/project_heart/{}_{}/\".format(fname,date))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prep_data_spark(df,cols):\n",
    "\n",
    "    df = df.filter(df.total_sessions > 1) \\\n",
    "    .na.fill('0').fillna(0) \\\n",
    "    .select(cols)\n",
    "    df.show()\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_data(df):\n",
    "\n",
    "    mask = df.iloc[:,1:-1].apply(lambda x : x>x.quantile(.99)).any(axis=1)\n",
    "    df_filtered = df.loc[~mask]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def prep_scale_data(df,cols,scaler=MinMaxScaler(inputCol=\"features\", \n",
    "                                              outputCol=\"scaledFeatures\")):\n",
    "\n",
    "    df = prep_data_spark(df,cols).rdd.map(lambda x:(Vectors.dense(x[1:]), x[0])) \\\n",
    "    .toDF([\"features\", \"browser_id\"])\n",
    "\n",
    "    scaled_df = scaler.fit(df) \\\n",
    "    .transform(df) \\\n",
    "    .drop('features') \\\n",
    "    .withColumnRenamed('scaledFeatures','features')\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "def find_optimum_nclusters(df,min_max_k):\n",
    "\n",
    "# Trains a k-means model.\n",
    "\n",
    "    wssse={}\n",
    "\n",
    "    K = range(min_max_k[0],min_max_k[1])\n",
    "\n",
    "    for i in K:\n",
    "\n",
    "        kmeans = KMeans().setK(i+1).setSeed(1)\n",
    "        model = kmeans.fit(df)\n",
    "\n",
    "        # Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "        wssse[i] = model.computeCost(df)\n",
    "        print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "        # Shows the result.\n",
    "        centers = model.clusterCenters()\n",
    "        print(\"Cluster Centers: \")\n",
    "        for center in centers:\n",
    "            print(center)\n",
    "\n",
    "    return K,wssse\n",
    "\n",
    "\n",
    "def kmeans_train(df,n_clusters):\n",
    "\n",
    "    kmeanModel = KMeans().setK(n_clusters).setSeed(1)\n",
    "    model = kmeanModel.fit(df)\n",
    "\n",
    "    return model\n",
    "\n",
    "def extract_kmeans(row):\n",
    "\n",
    "    return (row.browser_id, ) + tuple(row.features.toArray().tolist()) + (row.prediction, )\n",
    "\n",
    "def kmeans_pred(df,model,cols):\n",
    "\n",
    "    preds = model.transform(df)\n",
    "\n",
    "    predictionsdf = preds.rdd.map(extract_kmeans).toDF([col for col in cols]+['cluster'])\n",
    "\n",
    "    print(\"counts per cluster: \")\n",
    "    predictionsdf.groupBy(\"cluster\").count().show()\n",
    "\n",
    "    return predictionsdf\n",
    "\n",
    "def joined_result(df,df1,df2,n_clusters,cols):\n",
    "\n",
    "    df1 = df.join(df1, df1.browser_id == df.browser_id).select(df[\"*\"],df1[\"cluster\"])\n",
    "\n",
    "    df2 = df2.filter(df2.total_sessions == 1) \\\n",
    "    .na.fill('0').fillna(0) \\\n",
    "    .select(cols).withColumn(\"cluster\", lit(n_clusters)).union(df1) \n",
    "\n",
    "    return df1,df2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioural characteristics (total_attention, stdev_attention, vdays & section coverage):\n",
    "\n",
    "<a id='behave'></a>\n",
    "\n",
    "| Cluster # | Description                                                                                     |\n",
    "|-----------|-------------------------------------------------------------------------------------------------|\n",
    "| 0         | 4/5 days per week, medium/high section coverage and medium/high attention + std attention time  |\n",
    "| 1         | Low attention time, 2/3 times per week, low section coverage                                    |\n",
    "| 2         | High attention time, high std attention per session, 6 times per week amd high section coverage |\n",
    "| 3         | Very high attention time and std, 5-7 times per week, very high section coverage                |\n",
    "| 4         | Everyday browser, high section coverage, high attention                                         |\n",
    "| 5         | Once per week, low section coveration low attention time and high std attention                 |\n",
    "| 6         | Browsers with only one session                                                                  |\n",
    "\n",
    "[Back to index](#index)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#specify date locally and on cluster\n",
    "\n",
    "date='20190701_20190731'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "date='20190701_20190731'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load data to df\n",
    "\n",
    "df = load_df(access_key,secret_key,token,'behav_train')\n",
    "df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#produce scaled training set\n",
    "behav_cols = ['browser_id','total_attention_time','stddev_attention_time','vdays','section_coverage']\n",
    "behav_df_train_scaled = prep_scale_data(df,behav_cols,scaler=MinMaxScaler(inputCol=\"features\", \n",
    "                                               outputCol=\"scaledFeatures\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Find optimum k for behaviourial characteristics clustering\n",
    "#find_optimum_nclusters(behav_df_train_scaled,(6,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train K Means with optimum k\n",
    "\n",
    "behav_n_clusters = 6\n",
    "behav_kmeans_model = kmeans_train(behav_df_train_scaled,behav_n_clusters)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "behav_kmeans_model.save(\"s3://emr-project-heart/models/Behaviour/behav_kmeans_model_{}clusters_{}\".format(behav_n_clusters,date))\n",
    "#behav_kmeans_model = KMeansModel.load(\"s3://emr-project-heart/models/Behaviour/behav_kmeans_model_6clusters_20190401_20190407\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Predict K Means\n",
    "\n",
    "behav_preds = kmeans_pred(behav_df_train_scaled,behav_kmeans_model,behav_cols)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#join with sessions with less than one and also to get unscaled result\n",
    "\n",
    "behav_df_trained = prep_data_spark(df,behav_cols)\n",
    "\n",
    "behav_preds_joined,_= joined_result(behav_df_trained,behav_preds,df,behav_n_clusters,behav_cols)\n",
    "_,behav_full_joined = joined_result(behav_df_trained,behav_preds,df,behav_n_clusters,behav_cols)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#date= '20190729_20190804'\n",
    "name='behav_clusters'\n",
    "s3dir= \"s3a://ophan-temp-schema/data/dusvyat/project_heart\"\n",
    "behav_full_joined.write.parquet(\"{s3dir}/{date}/{name}/\".format(s3dir=s3dir,name=name,date=date),mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s3dir+\"/\"+date+\"/\"+name+\"/\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}